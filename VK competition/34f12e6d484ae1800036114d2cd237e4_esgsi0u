{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7101196-7de5-411f-8e42-e2e3c73c91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch tokenizers pandas fastparquet pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb8580-b3ed-4521-bd2a-ecccd6efd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.normalizers import Lowercase\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed609a0-979a-4d5e-8ff8-75e80b6dcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3954dd7-c0d4-4891-ae6e-fffbf2476d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[SEP]\"], vocab_size=320)\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    [f\"{row.ciphers} [SEP] {row.curves}\" for row in chain(train.itertuples(), test.itertuples())], \n",
    "    trainer=trainer\n",
    ")\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "PADDING_IDX = tokenizer.token_to_id(\"[PAD]\")\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274267cb-70a3-4ea3-9cae-81c63736ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        self.data = data\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[int, str, int]:\n",
    "        # Forget about UA for now\n",
    "        row = self.data.loc[idx]\n",
    "        return row.id, f\"{row.ciphers} [SEP] {row.curves}\", row.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892077e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        self.data = data\n",
    "        self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[int, str]:\n",
    "        # Forget about UA for now\n",
    "        row = self.data.loc[idx]\n",
    "        return row.id, f\"{row.ciphers} [SEP] {row.curves}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35ef2d-c3c7-490b-8e26-3faa7cfbeaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts: List[str]) -> torch.Tensor:\n",
    "    return torch.tensor([\n",
    "        _.ids for _ in tokenizer.encode_batch(texts, add_special_tokens=True)\n",
    "    ])\n",
    "\n",
    "def collate_to_train_batch(batch: List[Tuple[int, str, int]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    ids, texts, labels = zip(*batch)\n",
    "\n",
    "    ids_tensor = torch.tensor(ids, dtype=torch.long).view(-1, 1)\n",
    "    texts_tensor = tokenize(texts)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return ids_tensor, texts_tensor, label_tensor\n",
    "\n",
    "def collate_to_test_batch(batch: List[Tuple[int, str]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    ids, texts = zip(*batch)\n",
    "\n",
    "    ids_tensor = torch.tensor(ids, dtype=torch.long).view(-1, 1)\n",
    "    texts_tensor = tokenize(texts)\n",
    "\n",
    "    return ids_tensor, texts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334712b3-5afd-41f1-9b05-c5df5c62b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "    TrainDataset(train), batch_size=256, num_workers=16, collate_fn=collate_to_train_batch, pin_memory=False\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    TestDataset(test), batch_size=256, num_workers=16, collate_fn=collate_to_test_batch, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af73c6f-30ee-4d9b-9abc-ec4726691e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, padding_idx: int, vocab_size: int, embed_size: int, hidden_size: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=padding_idx)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(embed_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "        self.clf = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def get_embeds(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention/RNN/Transformers instead of mean()?\n",
    "        return self.embed(tensor).mean(dim=1)\n",
    "    \n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        embeds = self.dropout(self.get_embeds(tensor))\n",
    "        hiddens = self.hidden(embeds)\n",
    "        return self.clf(hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eef485-a52f-4f17-98bd-55e134db6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def training_step(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        _, X, y = batch\n",
    "        return self.criterion(self.model(X), y)\n",
    "    \n",
    "    def predict_step(self, batch: torch.Tensor, _) -> torch.Tensor:\n",
    "        ids, X, *_ = batch\n",
    "        return ids, torch.sigmoid(self.model(X))\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.005, weight_decay=0.05)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d015d-75e2-48c7-b215-78aa9251d75a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = LightningModel(\n",
    "    Model(vocab_size=VOCAB_SIZE, embed_size=128, hidden_size=96, padding_idx=PADDING_IDX, dropout=0.1)\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=8)\n",
    "trainer.fit(model=model, train_dataloaders=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ceced",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ids, probs = zip(*trainer.predict(model, dataloaders=test_dl))\n",
    "\n",
    "(\n",
    "    pd.DataFrame({\n",
    "        \"id\": torch.concat(ids).squeeze().numpy(),\n",
    "        \"is_bot\": torch.concat(probs).squeeze().numpy()\n",
    "    })\n",
    "    .to_csv(\"baseline_submission.csv\", index=None)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
